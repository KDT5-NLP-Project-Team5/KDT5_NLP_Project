{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "kingtotal = pd.read_csv('josuncsv/king_0.csv', sep=';')\n",
    "kingtotal\n",
    "\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def kiwi_preprocess(text):\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    return ' '.join([str(token) for token in tokens])\n",
    "\n",
    "kingtotal['text_kiwi'] = kingtotal['text'].apply(kiwi_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>text</th>\n",
       "      <th>text_kiwi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1년</td>\n",
       "      <td>10월</td>\n",
       "      <td>10일</td>\n",
       "      <td>임금이 수창궁(壽昌宮)에 거둥하였다.</td>\n",
       "      <td>Token(form='임금', tag='NNG', start=0, len=2) To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1년</td>\n",
       "      <td>10월</td>\n",
       "      <td>10일</td>\n",
       "      <td>이조 전서(吏曹典書)              유양(柳亮) ...</td>\n",
       "      <td>Token(form='이조', tag='NNP', start=14, len=2) T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1년</td>\n",
       "      <td>10월</td>\n",
       "      <td>11일</td>\n",
       "      <td>임금이 탄생일이므로 여러 신하들의 조하(朝賀)를 받고 사형(死刑)과 유형(流刑) 이...</td>\n",
       "      <td>Token(form='임금', tag='NNG', start=0, len=2) To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1년</td>\n",
       "      <td>10월</td>\n",
       "      <td>12일</td>\n",
       "      <td>우현보(禹玄寶)·이색(李穡)·설장수(偰長壽) 등 30인을 외방(外方)에 종편(從便...</td>\n",
       "      <td>Token(form='우현', tag='NNP', start=1, len=2) To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1년</td>\n",
       "      <td>10월</td>\n",
       "      <td>12일</td>\n",
       "      <td>공부 상정 도감(貢賦詳定都監)159)                  에서 상서(上書...</td>\n",
       "      <td>Token(form='공부', tag='NNG', start=0, len=2) To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>0</td>\n",
       "      <td>7년</td>\n",
       "      <td>윤5월</td>\n",
       "      <td>7일</td>\n",
       "      <td>사노(私奴) 오마대(吾麻大)가 그 아버지를 구타하였으므로, 명하여 이를 목 베게 하였다.</td>\n",
       "      <td>Token(form='사노', tag='NNG', start=0, len=2) To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>0</td>\n",
       "      <td>7년</td>\n",
       "      <td>윤5월</td>\n",
       "      <td>7일</td>\n",
       "      <td>비가 내리었다.</td>\n",
       "      <td>Token(form='비', tag='NNG', start=0, len=1) Tok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>0</td>\n",
       "      <td>7년</td>\n",
       "      <td>윤5월</td>\n",
       "      <td>9일</td>\n",
       "      <td>비가 내리었다.</td>\n",
       "      <td>Token(form='비', tag='NNG', start=0, len=1) Tok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>0</td>\n",
       "      <td>7년</td>\n",
       "      <td>윤5월</td>\n",
       "      <td>9일</td>\n",
       "      <td>임금이 서강(西江)에 거둥하였는데, 경상도의 조선(漕船) 20척이 도착 정박하였다.</td>\n",
       "      <td>Token(form='임금', tag='NNG', start=0, len=2) To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>0</td>\n",
       "      <td>3년</td>\n",
       "      <td>3월</td>\n",
       "      <td>19일</td>\n",
       "      <td>사신인 내사(內史) 노타내(盧他乃) 등이 중국 서울로 돌아가니, 임금이 여러 신하를...</td>\n",
       "      <td>Token(form='사신', tag='NNG', start=0, len=2) To...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2252 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label year month  day  \\\n",
       "0         0   1년   10월  10일   \n",
       "1         0   1년   10월  10일   \n",
       "2         0   1년   10월  11일   \n",
       "3         0   1년   10월  12일   \n",
       "4         0   1년   10월  12일   \n",
       "...     ...  ...   ...  ...   \n",
       "2247      0   7년   윤5월   7일   \n",
       "2248      0   7년   윤5월   7일   \n",
       "2249      0   7년   윤5월   9일   \n",
       "2250      0   7년   윤5월   9일   \n",
       "2251      0   3년    3월  19일   \n",
       "\n",
       "                                                   text  \\\n",
       "0                                  임금이 수창궁(壽昌宮)에 거둥하였다.   \n",
       "1                   이조 전서(吏曹典書)              유양(柳亮) ...   \n",
       "2     임금이 탄생일이므로 여러 신하들의 조하(朝賀)를 받고 사형(死刑)과 유형(流刑) 이...   \n",
       "3      우현보(禹玄寶)·이색(李穡)·설장수(偰長壽) 등 30인을 외방(外方)에 종편(從便...   \n",
       "4     공부 상정 도감(貢賦詳定都監)159)                  에서 상서(上書...   \n",
       "...                                                 ...   \n",
       "2247  사노(私奴) 오마대(吾麻大)가 그 아버지를 구타하였으므로, 명하여 이를 목 베게 하였다.   \n",
       "2248                                           비가 내리었다.   \n",
       "2249                                           비가 내리었다.   \n",
       "2250     임금이 서강(西江)에 거둥하였는데, 경상도의 조선(漕船) 20척이 도착 정박하였다.   \n",
       "2251  사신인 내사(內史) 노타내(盧他乃) 등이 중국 서울로 돌아가니, 임금이 여러 신하를...   \n",
       "\n",
       "                                              text_kiwi  \n",
       "0     Token(form='임금', tag='NNG', start=0, len=2) To...  \n",
       "1     Token(form='이조', tag='NNP', start=14, len=2) T...  \n",
       "2     Token(form='임금', tag='NNG', start=0, len=2) To...  \n",
       "3     Token(form='우현', tag='NNP', start=1, len=2) To...  \n",
       "4     Token(form='공부', tag='NNG', start=0, len=2) To...  \n",
       "...                                                 ...  \n",
       "2247  Token(form='사노', tag='NNG', start=0, len=2) To...  \n",
       "2248  Token(form='비', tag='NNG', start=0, len=1) Tok...  \n",
       "2249  Token(form='비', tag='NNG', start=0, len=1) Tok...  \n",
       "2250  Token(form='임금', tag='NNG', start=0, len=2) To...  \n",
       "2251  Token(form='사신', tag='NNG', start=0, len=2) To...  \n",
       "\n",
       "[2252 rows x 6 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kingtotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3607"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kingtotal.text_kiwi[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "tokenizer = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "2247    0\n",
       "2248    0\n",
       "2249    0\n",
       "2250    0\n",
       "2251    0\n",
       "Name: label, Length: 2252, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetDF = kingtotal.label\n",
    "targetDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    임금이 수창궁(壽昌宮)에 거둥하였다.\n",
       "1                     이조 전서(吏曹典書)              유양(柳亮) ...\n",
       "2       임금이 탄생일이므로 여러 신하들의 조하(朝賀)를 받고 사형(死刑)과 유형(流刑) 이...\n",
       "3        우현보(禹玄寶)·이색(李穡)·설장수(偰長壽) 등 30인을 외방(外方)에 종편(從便...\n",
       "4       공부 상정 도감(貢賦詳定都監)159)                  에서 상서(上書...\n",
       "                              ...                        \n",
       "2247    사노(私奴) 오마대(吾麻大)가 그 아버지를 구타하였으므로, 명하여 이를 목 베게 하였다.\n",
       "2248                                             비가 내리었다.\n",
       "2249                                             비가 내리었다.\n",
       "2250       임금이 서강(西江)에 거둥하였는데, 경상도의 조선(漕船) 20척이 도착 정박하였다.\n",
       "2251    사신인 내사(內史) 노타내(盧他乃) 등이 중국 서울로 돌아가니, 임금이 여러 신하를...\n",
       "Name: text, Length: 2252, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureDF = kingtotal.text\n",
    "featureDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(featureDF,\n",
    "                                                    targetDF,\n",
    "                                                    random_state=7,\n",
    "                                                    test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1645                  도평의사사(都評議使司)에 명하여, 서반 6품과 동반 7품 ...\n",
       "1336    이조(吏曹)에서 조종(祖宗)을 현양하고 배필(配匹)을 중히 하기를 청하였다. \"1....\n",
       "1681          이달에 큰비가 내려 경상도에 물로 손실된 밭이 거의 1만 결(結)이나 되었다.\n",
       "13                                         큰비가 오고 천둥이 쳤다.\n",
       "389                                왜적(倭賊)이 교동(喬桐)에 침구하였다.\n",
       "                              ...                        \n",
       "211                                      목가(木稼)083)  하였다.\n",
       "1603    간관(諫官)이 상언(上言)하였다. \"배표(拜表)하는 예를 초지(草地)에서 행할 수 ...\n",
       "537     내전에서 1백 8명의 중에게 밥을 먹이어 국사(國師)의 봉숭례(封崇禮)를 행하여, ...\n",
       "1220     합문 사인(閤門舍人) 권효(權曉)를 경상도에 보내서 최운해(崔雲海)에게 궁온(宮醞...\n",
       "175                   대사헌              남재(南在) 등이 상언(上言...\n",
       "Name: text, Length: 1576, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        # print(text)\n",
    "        yield iter(Okt.nouns(text))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_iter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vocab \u001b[38;5;241m=\u001b[39m build_vocab_from_iterator(yield_tokens(\u001b[43mtrain_iter\u001b[49m), specials\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m vocab\u001b[38;5;241m.\u001b[39mset_default_index(vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mlen\u001b[39m(vocab)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_iter' is not defined"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<pad>\", \"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_vocab_from_iterator() got an unexpected keyword argument 'key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_vocab_from_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43myield_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mspecials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<pad>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<unk>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_sort_key\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Add the 'key' argument\u001b[39;00m\n\u001b[1;32m      4\u001b[0m vocab\u001b[38;5;241m.\u001b[39mset_default_index(vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mlen\u001b[39m(vocab)\n",
      "\u001b[0;31mTypeError\u001b[0m: build_vocab_from_iterator() got an unexpected keyword argument 'key'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'kiwipiepy.Token' and 'kiwipiepy.Token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(tokens, key\u001b[38;5;241m=\u001b[39mtoken_sort_key) \u001b[38;5;66;03m# Sort tokens within each text example\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28miter\u001b[39m(tokens)\n\u001b[0;32m---> 10\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_vocab_from_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43myield_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<pad>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<unk>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m vocab\u001b[38;5;241m.\u001b[39mset_default_index(vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mlen\u001b[39m(vocab)\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch_NLP38/lib/python3.8/site-packages/torchtext/vocab/vocab_factory.py:104\u001b[0m, in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, min_freq, specials, special_first, max_tokens)\u001b[0m\n\u001b[1;32m    101\u001b[0m specials \u001b[38;5;241m=\u001b[39m specials \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# First sort by descending frequency, then lexicographically\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m sorted_by_freq_tuples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcounter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     ordered_dict \u001b[38;5;241m=\u001b[39m OrderedDict(sorted_by_freq_tuples)\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'kiwipiepy.Token' and 'kiwipiepy.Token'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 476, 22, 31, 5298]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab([\"<pad>\", \"<unk>\", \"here\", \"is\", \"an\", \"example\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1  # 0~3으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for label, text in batch:\n",
    "        label_list.append(label_pipeline(label))\n",
    "        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_iter, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_iter, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([3, 3, 1, 3, 1, 1, 1, 3, 0, 1, 3, 0, 0, 0, 3, 2, 0, 3, 0, 1, 2, 3, 0, 0,\n",
      "        3, 2, 1, 3, 1, 2, 0, 0, 3, 2, 0, 2, 0, 2, 3, 0, 3, 0, 0, 3, 1, 3, 0, 0,\n",
      "        2, 3, 2, 3, 0, 1, 3, 0, 2, 2, 0, 0, 2, 3, 3, 2]), tensor([ 100, 1169, 3631,  ...,    2,    2,    2]), tensor([   0,   62,   97,  153,  221,  261,  310,  348,  407,  459,  513,  553,\n",
      "         598,  635,  722,  768,  828,  875,  907,  989, 1053, 1099, 1149, 1167,\n",
      "        1210, 1251, 1305, 1357, 1383, 1426, 1454, 1484, 1539, 1561, 1594, 1631,\n",
      "        1664, 1701, 1749, 1787, 1809, 1843, 1868, 1914, 1953, 2004, 2042, 2086,\n",
      "        2129, 2197, 2240, 2296, 2334, 2380, 2430, 2497, 2542, 2595, 2631, 2666,\n",
      "        2715, 2757, 2792, 2829]))\n",
      "(tensor([2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 3, 1, 1, 0, 3,\n",
      "        0, 1, 0, 1, 0, 3, 2, 3, 0, 0, 2, 2, 1, 1, 1, 3]), tensor([ 870,   12,   84,  ..., 5499, 1330,    2]), tensor([   0,   28,   87,  138,  203,  246,  404,  529,  661,  679,  719,  783,\n",
      "         834,  885,  911,  933,  958, 1009, 1068, 1119, 1147, 1173, 1196, 1214,\n",
      "        1291, 1342, 1369, 1403, 1488, 1566, 1650, 1695, 1730, 1781, 1842, 1905,\n",
      "        1978, 2005, 2051, 2099, 2156, 2198, 2244, 2301, 2363, 2421, 2461, 2503,\n",
      "        2564, 2613, 2653, 2704, 2755, 2801, 2837, 2893, 2923, 2974, 3026, 3063,\n",
      "        3115, 3162, 3201, 3284]))\n"
     ]
    }
   ],
   "source": [
    "for a in train_loader:\n",
    "    print(a)\n",
    "    break\n",
    "for b in test_loader:\n",
    "    print(b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_class : 4    vocab_size : 95812\n"
     ]
    }
   ],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 300\n",
    "num_epochs = 5\n",
    "lr = 0.001\n",
    "\n",
    "print(f\"num_class : {num_class}    vocab_size : {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        hidden_dim,\n",
    "        embedding_dim,\n",
    "        n_layer,\n",
    "        dropout=0.5,\n",
    "        bidirectional=True,\n",
    "        model_type=\"lstm\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab, embedding_dim=embedding_dim, padding_idx=0\n",
    "        )\n",
    "        if model_type == \"rnn\":\n",
    "            self.model = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layer,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        elif model_type == \"lstm\":\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layer,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2, num_class)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab)\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layer = 2\n",
    "\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab=n_vocab, \n",
    "    hidden_dim=hidden_dim, \n",
    "    embedding_dim=embedding_dim, \n",
    "    n_layer=n_layer\n",
    ").to(device)\n",
    "\n",
    "classifier = TextClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_class=num_class\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "TextClassifier                           --\n",
       "├─EmbeddingBag: 1-1                      28,743,600\n",
       "├─RNN: 1-2                               23,424\n",
       "├─Linear: 1-3                            1,204\n",
       "=================================================================\n",
       "Total params: 28,768,228\n",
       "Trainable params: 28,768,228\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for idx, (labels, texts, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts, offsets)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        acc = (outputs.argmax(1) == labels).float().sum()\n",
    "        total_acc += acc.item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    return total_loss / total_samples, total_acc / total_samples\n",
    "\n",
    "def evaluate(model, dataloader, is_training=False):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for labels, texts, offsets in dataloader:\n",
    "            outputs = model(texts, offsets)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            acc = (outputs.argmax(1) == labels).float().sum()\n",
    "            total_acc += acc.item()\n",
    "            total_samples += labels.size(0)\n",
    "    return total_loss / total_samples, total_acc / total_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text), dtype=torch.int64).to(device)\n",
    "        text = text.unsqueeze(0)\n",
    "        offsets = torch.tensor([0]).to(device)\n",
    "        predicted_label = model(text, offsets)\n",
    "        return predicted_label.argmax(1).item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.0005, Train Acc: 0.9889, Valid Loss: 0.4608, Valid Acc: 0.8996\n",
      "Epoch: 2, Train Loss: 0.0005, Train Acc: 0.9904, Valid Loss: 0.5020, Valid Acc: 0.8957\n",
      "Epoch: 3, Train Loss: 0.0004, Train Acc: 0.9920, Valid Loss: 0.5397, Valid Acc: 0.8939\n",
      "Epoch: 4, Train Loss: 0.0004, Train Acc: 0.9922, Valid Loss: 0.5794, Valid Acc: 0.8925\n",
      "Epoch: 5, Train Loss: 0.0003, Train Acc: 0.9933, Valid Loss: 0.6172, Valid Acc: 0.8908\n",
      "Test Loss: 0.4608, Test Acc: 0.8996\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(classifier, train_loader)\n",
    "    valid_loss, valid_acc = evaluate(classifier, test_loader)  # valid_loader 대신 test_loader 사용\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(classifier.state_dict(), 'best_model.pth')\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}')\n",
    "\n",
    "# 모델 평가\n",
    "classifier.load_state_dict(torch.load('best_model.pth'))\n",
    "test_loss, test_acc = evaluate(classifier, test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
