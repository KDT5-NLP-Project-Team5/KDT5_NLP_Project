{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmbeddingBag\n",
    "1. Vocab을 가져와 EmbeddingBag을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이터레이터 토큰화 함수\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "Okt = Okt()\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        # print(text)\n",
    "        yield iter(Okt.nouns(text))\n",
    "\n",
    "# next(yield_tokens(file3['text'])) # 확인함\n",
    "\n",
    "# 파일 읽기\n",
    "\n",
    "file_path = '../DATA/king_3.csv'\n",
    "file = pd.read_csv(file_path, encoding='utf-8', sep=';')\n",
    "data = file['text'].values\n",
    "\n",
    "# vocab 생성\n",
    "vocab = build_vocab_from_iterator(yield_tokens(data), specials=[\"<unk>\"])\n",
    "# vocab 확인\n",
    "print('길이 :', len(vocab))\n",
    "print(vocab.get_itos()[:10])\n",
    "\n",
    "# <unk> 인덱스를 0으로 설정\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 텍스트 -> 정수 인코딩\n",
    "# - lambda 함수 인스턴스 생성\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "    # x를 토큰화한 후, 단어 사전에 있는 단어들을 정수로 변환\n",
    "\n",
    "# 레이블 -> 정수 인코딩\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    batch : (label, text)\n",
    "    \"\"\"\n",
    "    label_list, text_list, offsets = [], [], [0]  # offsets : 텍스트 길이\n",
    "\n",
    "    for _label, _text in batch:\n",
    "        # (1) 라벨 정수 인코딩\n",
    "        label_list.append(label_pipeline(_label))  \n",
    "        # (2) 텍스트 정수 인코딩\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        \n",
    "        text_list.append(processed_text)  # 텍스트 리스트에 추가\n",
    "        offsets.append(processed_text.size(0))  # 텍스트 길이 추가\n",
    "\n",
    "    # tensor 변환\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "# 데이터 로더 생성\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    \"\"\"\n",
    "    텍스트 분류 모델 정의; nn.EmbeddingBag 사용 연습\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, VOCAB_SIZE, EMBED_DIM, HIDDEN_SIZE, NUM_CLASS):\n",
    "        \"\"\"_summary_ : 모델 초기화 함수\n",
    "        \n",
    "        Args:\n",
    "            VOCAB_SIZE (int): 어휘 사전 크기\n",
    "            EMBED_DIM (int): 임베딩 차원 (단어 벡터 차원)\n",
    "            HIDDEN_SIZE (int): 은닉층 크기\n",
    "            NUM_CLASS (int): 분류 클래스 개수\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(VOCAB_SIZE, EMBED_DIM, sparse=False)\n",
    "        self.fc = nn.Linear(EMBED_DIM, NUM_CLASS)\n",
    "        self.init_weights()     # 가중치 초기화 : 학습 전에 사용\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"_summary_ : 가중치 초기화 함수\n",
    "        \n",
    "        .unform_() : 균등 분포로 weight 초기화\n",
    "        .zero_() : 0으로 초기화\n",
    "        -initrange ~ initrange 사이의 값으로 초기화\n",
    "        \"\"\"\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOC2VEC 도 해볼까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
