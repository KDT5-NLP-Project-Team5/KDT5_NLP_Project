{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmbeddingBag\n",
    "1. Vocab을 가져와 EmbeddingBag을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    \"\"\"\n",
    "    텍스트 분류 모델 정의; nn.EmbeddingBag 사용 연습\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, VOCAB_SIZE, EMBED_DIM, HIDDEN_SIZE, NUM_CLASS):\n",
    "        \"\"\"_summary_ : 모델 초기화 함수\n",
    "        \n",
    "        Args:\n",
    "            VOCAB_SIZE (int): 어휘 사전 크기\n",
    "            EMBED_DIM (int): 임베딩 차원 (단어 벡터 차원)\n",
    "            HIDDEN_SIZE (int): 은닉층 크기\n",
    "            NUM_CLASS (int): 분류 클래스 개수\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(VOCAB_SIZE, EMBED_DIM, sparse=False)\n",
    "        self.fc = nn.Linear(EMBED_DIM, NUM_CLASS)\n",
    "        self.init_weights()     # 가중치 초기화 : 학습 전에 사용\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"_summary_ : 가중치 초기화 함수\n",
    "        \n",
    "        .unform_() : 균등 분포로 weight 초기화\n",
    "        .zero_() : 0으로 초기화\n",
    "        -initrange ~ initrange 사이의 값으로 초기화\n",
    "        \"\"\"\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        \"\"\"_summary_ : 모델 forward 함수\n",
    "        embedding할 데이터를 모두 받고\n",
    "        offsets 기준으로 끊어 embedding\n",
    "        \n",
    "        Args:\n",
    "            text (Tensor) : 텍스트 데이터\n",
    "            offsets (Tensor) : 각 시퀀스의 시작 인덱스\n",
    "        \n",
    "        return : embedding return : \n",
    "        \"\"\"\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
