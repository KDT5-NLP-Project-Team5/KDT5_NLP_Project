{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word2vec\n",
    "- 크롤링 -> 데이터 전처리(->csv) -> 전처리한 csv에 text를 토크나이즈 -> vocab -> embedding -> 모델(RNN이나 CNN) 학습 -> 예측값\n",
    "-     시명님, 진우님 완료                앞 뒤로 불용어 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 각 왕 시대에 가장 많이 언급된 이름, 혹은 관직 아니면 기관\n",
    "- word2vec로 왕과 언급된 이름의 연관도를 예측 %로 나타내기\n",
    "- 이름이 없으면 관직, 관직이 없으면 기관\n",
    "- gpt의 word2vec 코드\n",
    "- 데이터 -> 파일 -> 토크나이징 -> vocab -> embedding ~ embedding bag -> model\n",
    "-                                           (라벨링)             (word2vec)\n",
    "\n",
    "- 각 세대 왕의 실록의 단어 빈도를 체크 -> 그 중에서 언급된 이름 중 가장 많이 언급된 이름을 선택 -> 그 이름과 왕과 연관이 있는지 유사도 %를 나타내보기\n",
    "- ex) 태조를 적으면 정도전이 %나오는지 확인\n",
    "- 결론: 그 세대의 왕과 그 단어(이름, 관직, 기관)가 얼마나 연관되어있는지 퍼센트를 보고 역사적 기록을 그 만한 비중인지 확인해보기(맞는지 아닌지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>10년</td>\n",
       "      <td>10월</td>\n",
       "      <td>10일</td>\n",
       "      <td>문신(文臣)과 종신(宗臣)에게 사후(射帿)573)  하기를 명하였는데, 문신으로 활...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>10년</td>\n",
       "      <td>10월</td>\n",
       "      <td>11일</td>\n",
       "      <td>지평(持平) 이성효(李性孝)가 상소했는데, 대략 이르기를, \"전하께서 10년 동안...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>10년</td>\n",
       "      <td>10월</td>\n",
       "      <td>12일</td>\n",
       "      <td>이보욱(李普昱)을 사간으로, 임정(任珽)을 교리로, 정형복(鄭亨復)을 부교리로, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>10년</td>\n",
       "      <td>10월</td>\n",
       "      <td>13일</td>\n",
       "      <td>정필녕(鄭必寧)을 승지로, 민응수(閔應洙)를 부제학으로 삼았다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>10년</td>\n",
       "      <td>10월</td>\n",
       "      <td>13일</td>\n",
       "      <td>원계군(原溪君) 엽(㮿)이 상소하였는데, 대략 이르기를, \"고(故) 종신(宗臣) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36716</th>\n",
       "      <td>20</td>\n",
       "      <td>즉위년</td>\n",
       "      <td>9월</td>\n",
       "      <td>5일</td>\n",
       "      <td>이보다 앞서 함경 감사(咸鏡監司) 이의만(李宜晩)이 본도(本道)의 살옥(殺獄)에 대...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36717</th>\n",
       "      <td>20</td>\n",
       "      <td>즉위년</td>\n",
       "      <td>9월</td>\n",
       "      <td>6일</td>\n",
       "      <td>대사헌(大司憲) 이명언(李明彦)이 말하기를, \"현종 대왕(顯宗大王)의 상(喪) 때...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36718</th>\n",
       "      <td>20</td>\n",
       "      <td>즉위년</td>\n",
       "      <td>9월</td>\n",
       "      <td>7일</td>\n",
       "      <td>이때에 임금이 감기를 앓은 지 오래 되어도 낫지 아니하였는데도 오히려 아침 저녁의 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36719</th>\n",
       "      <td>20</td>\n",
       "      <td>즉위년</td>\n",
       "      <td>9월</td>\n",
       "      <td>7일</td>\n",
       "      <td>임금이 사관(史官)을 보내어 찬선(贊善) 정제두(鄭齊斗)를 유소(諭召)하여 함께 오...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36720</th>\n",
       "      <td>20</td>\n",
       "      <td>10년</td>\n",
       "      <td>10월</td>\n",
       "      <td>10일</td>\n",
       "      <td>임금이 춘당대(春塘臺)에 친림(親臨)하여 유생을 시험 보여 이명곤(李命坤) 등 5인...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36721 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label year month  day  \\\n",
       "0         20  10년   10월  10일   \n",
       "1         20  10년   10월  11일   \n",
       "2         20  10년   10월  12일   \n",
       "3         20  10년   10월  13일   \n",
       "4         20  10년   10월  13일   \n",
       "...      ...  ...   ...  ...   \n",
       "36716     20  즉위년    9월   5일   \n",
       "36717     20  즉위년    9월   6일   \n",
       "36718     20  즉위년    9월   7일   \n",
       "36719     20  즉위년    9월   7일   \n",
       "36720     20  10년   10월  10일   \n",
       "\n",
       "                                                    text  \n",
       "0      문신(文臣)과 종신(宗臣)에게 사후(射帿)573)  하기를 명하였는데, 문신으로 활...  \n",
       "1       지평(持平) 이성효(李性孝)가 상소했는데, 대략 이르기를, \"전하께서 10년 동안...  \n",
       "2       이보욱(李普昱)을 사간으로, 임정(任珽)을 교리로, 정형복(鄭亨復)을 부교리로, ...  \n",
       "3                    정필녕(鄭必寧)을 승지로, 민응수(閔應洙)를 부제학으로 삼았다.  \n",
       "4       원계군(原溪君) 엽(㮿)이 상소하였는데, 대략 이르기를, \"고(故) 종신(宗臣) ...  \n",
       "...                                                  ...  \n",
       "36716  이보다 앞서 함경 감사(咸鏡監司) 이의만(李宜晩)이 본도(本道)의 살옥(殺獄)에 대...  \n",
       "36717   대사헌(大司憲) 이명언(李明彦)이 말하기를, \"현종 대왕(顯宗大王)의 상(喪) 때...  \n",
       "36718  이때에 임금이 감기를 앓은 지 오래 되어도 낫지 아니하였는데도 오히려 아침 저녁의 ...  \n",
       "36719  임금이 사관(史官)을 보내어 찬선(贊善) 정제두(鄭齊斗)를 유소(諭召)하여 함께 오...  \n",
       "36720  임금이 춘당대(春塘臺)에 친림(親臨)하여 유생을 시험 보여 이명곤(李命坤) 등 5인...  \n",
       "\n",
       "[36721 rows x 5 columns]"
      ]
     },
     "execution_count": 1093,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "trainDF = pd.read_table('./data/KING/king_20.csv', encoding='utf-8', sep=';')\n",
    "trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainDF => (36721, 2)\n"
     ]
    }
   ],
   "source": [
    "# 불필요 컬럼 삭제\n",
    "trainDF.drop(columns=['year', 'month', 'day'], inplace=True)\n",
    "print(f'trainDF => {trainDF.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "20    36721\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 1095,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 클래스 비율 체크\n",
    "trainDF['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colnumns => Index(['label', 'text'], dtype='object')\n",
      "textDF => (36721, 1), labelSR => (36721,)\n"
     ]
    }
   ],
   "source": [
    "# 피쳐와 레이블 분리\n",
    "print(f'colnumns => {trainDF.columns}')\n",
    "\n",
    "textDF = trainDF.text.to_frame()\n",
    "labelSR=trainDF.label\n",
    "\n",
    "print(f'textDF => {textDF.shape}, labelSR => {labelSR.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>문신(文臣)과 종신(宗臣)에게 사후(射帿)573)  하기를 명하였는데, 문신으로 활...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>지평(持平) 이성효(李性孝)가 상소했는데, 대략 이르기를, \"전하께서 10년 동안...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이보욱(李普昱)을 사간으로, 임정(任珽)을 교리로, 정형복(鄭亨復)을 부교리로, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>정필녕(鄭必寧)을 승지로, 민응수(閔應洙)를 부제학으로 삼았다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>원계군(原溪君) 엽(㮿)이 상소하였는데, 대략 이르기를, \"고(故) 종신(宗臣) ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  문신(文臣)과 종신(宗臣)에게 사후(射帿)573)  하기를 명하였는데, 문신으로 활...\n",
       "1   지평(持平) 이성효(李性孝)가 상소했는데, 대략 이르기를, \"전하께서 10년 동안...\n",
       "2   이보욱(李普昱)을 사간으로, 임정(任珽)을 교리로, 정형복(鄭亨復)을 부교리로, ...\n",
       "3                정필녕(鄭必寧)을 승지로, 민응수(閔應洙)를 부제학으로 삼았다.\n",
       "4   원계군(原溪君) 엽(㮿)이 상소하였는데, 대략 이르기를, \"고(故) 종신(宗臣) ..."
      ]
     },
     "execution_count": 1097,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글과 공백을 제외하고 모두 제거\n",
    "textDF['text'] = textDF['text'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣]', '')\n",
    "textDF[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 정의\n",
    "stop_words = './data/stop_word.txt'\n",
    "\n",
    "with open(stop_words, encoding='utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "stopwords = set(lines) # set을 이용하여 혹시 모를 중복을 피함\n",
    "\n",
    "# 형태소 분석기 Okt를 사용한 토큰화 작업\n",
    "okt = Okt()\n",
    "\n",
    "tokenized_data = []\n",
    "for sentence in textDF['text']:\n",
    "    tokenized_sentence = okt.morphs(sentence) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if word not in stopwords and not re.match(r'[^ 가-힣]+$', word)] # 불용어와 한자 제거\n",
    "    tokenized_data.append(stopwords_removed_sentence)\n",
    "    #tokenized_data.extend(stopwords_removed_sentence) # extend를 사용하여 리스트를 확장합니다. 표 만들 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 빈도 수 시각화\n",
    "# word_counts = Counter(tokenized_data)\n",
    "\n",
    "# # 상위 10개 단어 추출\n",
    "# top_words = dict(word_counts.most_common(10))\n",
    "\n",
    "# # 막대 그래프 그리기\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# plt.bar(top_words.keys(), top_words.values(), color='red')\n",
    "# plt.xlabel('단어')\n",
    "# plt.ylabel('빈도수')\n",
    "# plt.title('세조 시대 단어 빈도 상위 10개')\n",
    "# plt.xticks()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 12:25:47,784 : INFO : collecting all words and their counts\n",
      "2024-04-05 12:25:47,784 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-04-05 12:25:47,916 : INFO : PROGRESS: at sentence #10000, processed 745822 words, keeping 43232 word types\n",
      "2024-04-05 12:25:48,025 : INFO : PROGRESS: at sentence #20000, processed 1390881 words, keeping 59007 word types\n",
      "2024-04-05 12:25:48,121 : INFO : PROGRESS: at sentence #30000, processed 1883414 words, keeping 67065 word types\n",
      "2024-04-05 12:25:48,214 : INFO : collected 75871 word types from a corpus of 2443565 raw words and 36721 sentences\n",
      "2024-04-05 12:25:48,214 : INFO : Creating a fresh vocabulary\n",
      "2024-04-05 12:25:48,512 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 75871 unique words (100.00% of original 75871, drops 0)', 'datetime': '2024-04-05T12:25:48.512938', 'gensim': '4.3.2', 'python': '3.8.19 (default, Mar 20 2024, 19:55:45) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-04-05 12:25:48,512 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 2443565 word corpus (100.00% of original 2443565, drops 0)', 'datetime': '2024-04-05T12:25:48.512938', 'gensim': '4.3.2', 'python': '3.8.19 (default, Mar 20 2024, 19:55:45) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-04-05 12:25:48,858 : INFO : deleting the raw counts dictionary of 75871 items\n",
      "2024-04-05 12:25:48,858 : INFO : sample=0.001 downsamples 18 most-common words\n",
      "2024-04-05 12:25:48,858 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2349166.169651654 word corpus (96.1%% of prior 2443565)', 'datetime': '2024-04-05T12:25:48.858807', 'gensim': '4.3.2', 'python': '3.8.19 (default, Mar 20 2024, 19:55:45) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-04-05 12:25:49,330 : INFO : estimated required memory for 75871 words and 100 dimensions: 98632300 bytes\n",
      "2024-04-05 12:25:49,330 : INFO : resetting layer weights\n",
      "2024-04-05 12:25:49,360 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-04-05T12:25:49.360960', 'gensim': '4.3.2', 'python': '3.8.19 (default, Mar 20 2024, 19:55:45) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'build_vocab'}\n",
      "2024-04-05 12:25:49,360 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 75871 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-04-05T12:25:49.360960', 'gensim': '4.3.2', 'python': '3.8.19 (default, Mar 20 2024, 19:55:45) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2024-04-05 12:25:50,394 : INFO : EPOCH 0 - PROGRESS: at 16.55% examples, 384180 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:25:51,416 : INFO : EPOCH 0 - PROGRESS: at 29.84% examples, 386898 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:25:52,414 : INFO : EPOCH 0 - PROGRESS: at 44.40% examples, 386128 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:25:53,410 : INFO : EPOCH 0 - PROGRESS: at 67.12% examples, 386291 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:25:54,463 : INFO : EPOCH 0 - PROGRESS: at 88.63% examples, 388504 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:25:55,401 : INFO : EPOCH 0: training on 2443565 raw words (2349177 effective words) took 6.0s, 389020 effective words/s\n",
      "2024-04-05 12:25:56,424 : INFO : EPOCH 1 - PROGRESS: at 16.55% examples, 388131 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:25:57,440 : INFO : EPOCH 1 - PROGRESS: at 30.06% examples, 395531 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:25:58,449 : INFO : EPOCH 1 - PROGRESS: at 45.40% examples, 393991 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:25:59,447 : INFO : EPOCH 1 - PROGRESS: at 69.13% examples, 396758 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:00,465 : INFO : EPOCH 1 - PROGRESS: at 89.28% examples, 397535 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:01,297 : INFO : EPOCH 1: training on 2443565 raw words (2349394 effective words) took 5.9s, 398556 effective words/s\n",
      "2024-04-05 12:26:02,380 : INFO : EPOCH 2 - PROGRESS: at 17.46% examples, 396268 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:03,380 : INFO : EPOCH 2 - PROGRESS: at 31.47% examples, 405387 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:04,383 : INFO : EPOCH 2 - PROGRESS: at 50.71% examples, 416918 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:05,396 : INFO : EPOCH 2 - PROGRESS: at 75.87% examples, 417115 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:06,430 : INFO : EPOCH 2 - PROGRESS: at 93.40% examples, 417526 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:06,913 : INFO : EPOCH 2: training on 2443565 raw words (2349101 effective words) took 5.6s, 418377 effective words/s\n",
      "2024-04-05 12:26:07,946 : INFO : EPOCH 3 - PROGRESS: at 16.55% examples, 390916 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:08,946 : INFO : EPOCH 3 - PROGRESS: at 30.69% examples, 406443 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:09,946 : INFO : EPOCH 3 - PROGRESS: at 47.64% examples, 408404 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:10,961 : INFO : EPOCH 3 - PROGRESS: at 70.45% examples, 405208 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:11,977 : INFO : EPOCH 3 - PROGRESS: at 89.89% examples, 402104 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:12,986 : INFO : EPOCH 3 - PROGRESS: at 98.91% examples, 378931 words/s, in_qsize 6, out_qsize 0\n",
      "2024-04-05 12:26:13,102 : INFO : EPOCH 3: training on 2443565 raw words (2349490 effective words) took 6.2s, 380800 effective words/s\n",
      "2024-04-05 12:26:14,142 : INFO : EPOCH 4 - PROGRESS: at 14.57% examples, 337667 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:15,182 : INFO : EPOCH 4 - PROGRESS: at 28.19% examples, 358895 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:16,297 : INFO : EPOCH 4 - PROGRESS: at 40.24% examples, 338294 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:17,315 : INFO : EPOCH 4 - PROGRESS: at 57.33% examples, 327109 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:18,400 : INFO : EPOCH 4 - PROGRESS: at 79.26% examples, 333414 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:19,467 : INFO : EPOCH 4 - PROGRESS: at 94.97% examples, 342441 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:19,900 : INFO : EPOCH 4: training on 2443565 raw words (2349083 effective words) took 6.8s, 345742 effective words/s\n",
      "2024-04-05 12:26:20,915 : INFO : EPOCH 5 - PROGRESS: at 15.24% examples, 363186 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:21,925 : INFO : EPOCH 5 - PROGRESS: at 28.19% examples, 368731 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:22,931 : INFO : EPOCH 5 - PROGRESS: at 39.92% examples, 354940 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:23,985 : INFO : EPOCH 5 - PROGRESS: at 60.25% examples, 353893 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:24,997 : INFO : EPOCH 5 - PROGRESS: at 82.00% examples, 357017 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:26,001 : INFO : EPOCH 5 - PROGRESS: at 95.85% examples, 360006 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:26,428 : INFO : EPOCH 5: training on 2443565 raw words (2349086 effective words) took 6.5s, 360198 effective words/s\n",
      "2024-04-05 12:26:27,431 : INFO : EPOCH 6 - PROGRESS: at 14.57% examples, 347170 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:28,445 : INFO : EPOCH 6 - PROGRESS: at 25.63% examples, 335047 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:29,462 : INFO : EPOCH 6 - PROGRESS: at 39.41% examples, 351390 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:30,472 : INFO : EPOCH 6 - PROGRESS: at 60.25% examples, 357377 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:31,505 : INFO : EPOCH 6 - PROGRESS: at 81.26% examples, 353449 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:32,544 : INFO : EPOCH 6 - PROGRESS: at 89.74% examples, 331884 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:33,536 : INFO : EPOCH 6: training on 2443565 raw words (2349157 effective words) took 7.1s, 330678 effective words/s\n",
      "2024-04-05 12:26:34,562 : INFO : EPOCH 7 - PROGRESS: at 13.62% examples, 315185 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:35,607 : INFO : EPOCH 7 - PROGRESS: at 26.11% examples, 333096 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:36,612 : INFO : EPOCH 7 - PROGRESS: at 39.04% examples, 343595 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:37,628 : INFO : EPOCH 7 - PROGRESS: at 59.44% examples, 347592 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:38,661 : INFO : EPOCH 7 - PROGRESS: at 81.49% examples, 351164 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:39,682 : INFO : EPOCH 7 - PROGRESS: at 93.76% examples, 350328 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:40,194 : INFO : EPOCH 7: training on 2443565 raw words (2349228 effective words) took 6.7s, 352769 effective words/s\n",
      "2024-04-05 12:26:41,211 : INFO : EPOCH 8 - PROGRESS: at 13.62% examples, 319959 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:42,255 : INFO : EPOCH 8 - PROGRESS: at 26.11% examples, 336052 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:43,261 : INFO : EPOCH 8 - PROGRESS: at 39.04% examples, 344870 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:44,305 : INFO : EPOCH 8 - PROGRESS: at 58.63% examples, 342943 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:45,328 : INFO : EPOCH 8 - PROGRESS: at 79.26% examples, 344468 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:46,378 : INFO : EPOCH 8 - PROGRESS: at 92.06% examples, 340601 words/s, in_qsize 6, out_qsize 1\n",
      "2024-04-05 12:26:47,081 : INFO : EPOCH 8: training on 2443565 raw words (2349090 effective words) took 6.9s, 341521 effective words/s\n",
      "2024-04-05 12:26:48,094 : INFO : EPOCH 9 - PROGRESS: at 13.94% examples, 325479 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:49,195 : INFO : EPOCH 9 - PROGRESS: at 21.57% examples, 272073 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:50,231 : INFO : EPOCH 9 - PROGRESS: at 31.69% examples, 270405 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:51,230 : INFO : EPOCH 9 - PROGRESS: at 44.40% examples, 284149 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:52,240 : INFO : EPOCH 9 - PROGRESS: at 63.76% examples, 293120 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:53,231 : INFO : EPOCH 9 - PROGRESS: at 82.73% examples, 300491 words/s, in_qsize 8, out_qsize 0\n",
      "2024-04-05 12:26:54,260 : INFO : EPOCH 9 - PROGRESS: at 94.97% examples, 303800 words/s, in_qsize 7, out_qsize 0\n",
      "2024-04-05 12:26:54,762 : INFO : EPOCH 9: training on 2443565 raw words (2349145 effective words) took 7.7s, 305805 effective words/s\n",
      "2024-04-05 12:26:54,762 : INFO : Word2Vec lifecycle event {'msg': 'training on 24435650 raw words (23491951 effective words) took 65.4s, 359168 effective words/s', 'datetime': '2024-04-05T12:26:54.762837', 'gensim': '4.3.2', 'python': '3.8.19 (default, Mar 20 2024, 19:55:45) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2024-04-05 12:26:54,762 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=75871, vector_size=100, alpha=0.025>', 'datetime': '2024-04-05T12:26:54.762837', 'gensim': '4.3.2', 'python': '3.8.19 (default, Mar 20 2024, 19:55:45) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# sg=1 skip-gram: 입력을 받아서 주변 단어를 예측\n",
    "# sg=0 CBoW: 주변에 있는 단어를 가지고 중간에 있는 단어를 예측\n",
    "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4, sg=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75871, 100)"
      ]
     },
     "execution_count": 1101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('어계선', 0.7416821122169495),\n",
       " ('함격', 0.7394095659255981),\n",
       " ('가로막아야', 0.7360052466392517),\n",
       " ('뽑혔고', 0.7337253093719482),\n",
       " ('였었고', 0.7290489673614502),\n",
       " ('되었었으므로', 0.7288016676902771),\n",
       " ('종정', 0.7287483215332031),\n",
       " ('가리킴', 0.7188725471496582),\n",
       " ('올려주었고', 0.7186225652694702),\n",
       " ('차천로', 0.7170872092247009)]"
      ]
     },
     "execution_count": 1102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"영조\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35901135\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity(\"영조\", \"이인좌\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
